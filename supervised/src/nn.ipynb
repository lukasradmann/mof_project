{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MOFname</th>\n",
       "      <th>CO2_uptake_P0.15bar_T298K [mmol/g]</th>\n",
       "      <th>volume [A^3]</th>\n",
       "      <th>weight [u]</th>\n",
       "      <th>surface_area [m^2/g]</th>\n",
       "      <th>void_fraction</th>\n",
       "      <th>void_volume [cm^3/g]</th>\n",
       "      <th>largest_free_sphere_diameter [A]</th>\n",
       "      <th>largest_included_sphere_diameter [A]</th>\n",
       "      <th>metal_linker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>str_m5_o16_o16_sra_sym.77</td>\n",
       "      <td>5.955197</td>\n",
       "      <td>2473.186302</td>\n",
       "      <td>1493.01184</td>\n",
       "      <td>613.30</td>\n",
       "      <td>0.14835</td>\n",
       "      <td>0.1480</td>\n",
       "      <td>4.61370</td>\n",
       "      <td>4.61370</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>str_m5_o16_o16_sra_sym.37</td>\n",
       "      <td>5.715251</td>\n",
       "      <td>2419.885159</td>\n",
       "      <td>1444.79680</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16099</td>\n",
       "      <td>0.1624</td>\n",
       "      <td>4.10210</td>\n",
       "      <td>4.10055</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>str_m5_o13_o18_sra_sym.149</td>\n",
       "      <td>5.524486</td>\n",
       "      <td>2514.627698</td>\n",
       "      <td>1504.68312</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17980</td>\n",
       "      <td>0.1810</td>\n",
       "      <td>4.24711</td>\n",
       "      <td>4.24603</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>str_m5_o2_o18_sra_sym.4</td>\n",
       "      <td>5.517258</td>\n",
       "      <td>2128.612920</td>\n",
       "      <td>1424.81024</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.10245</td>\n",
       "      <td>0.0922</td>\n",
       "      <td>4.11419</td>\n",
       "      <td>3.63925</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>str_m5_o16_o16_sra_sym.31</td>\n",
       "      <td>5.451593</td>\n",
       "      <td>2415.251225</td>\n",
       "      <td>1436.90552</td>\n",
       "      <td>1347.04</td>\n",
       "      <td>0.15634</td>\n",
       "      <td>0.1583</td>\n",
       "      <td>4.37077</td>\n",
       "      <td>4.16451</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      MOFname  CO2_uptake_P0.15bar_T298K [mmol/g]  \\\n",
       "0   str_m5_o16_o16_sra_sym.77                            5.955197   \n",
       "1   str_m5_o16_o16_sra_sym.37                            5.715251   \n",
       "2  str_m5_o13_o18_sra_sym.149                            5.524486   \n",
       "3     str_m5_o2_o18_sra_sym.4                            5.517258   \n",
       "4   str_m5_o16_o16_sra_sym.31                            5.451593   \n",
       "\n",
       "   volume [A^3]  weight [u]  surface_area [m^2/g]  void_fraction  \\\n",
       "0   2473.186302  1493.01184                613.30        0.14835   \n",
       "1   2419.885159  1444.79680                  0.00        0.16099   \n",
       "2   2514.627698  1504.68312                  0.00        0.17980   \n",
       "3   2128.612920  1424.81024                  0.00        0.10245   \n",
       "4   2415.251225  1436.90552               1347.04        0.15634   \n",
       "\n",
       "   void_volume [cm^3/g]  largest_free_sphere_diameter [A]  \\\n",
       "0                0.1480                           4.61370   \n",
       "1                0.1624                           4.10210   \n",
       "2                0.1810                           4.24711   \n",
       "3                0.0922                           4.11419   \n",
       "4                0.1583                           4.37077   \n",
       "\n",
       "   largest_included_sphere_diameter [A]  metal_linker  \n",
       "0                               4.61370             9  \n",
       "1                               4.10055             9  \n",
       "2                               4.24603             9  \n",
       "3                               3.63925             9  \n",
       "4                               4.16451             9  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/processed_MOFs.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['MOFname'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    }
   ],
   "source": [
    "# Sample 10% of the data\n",
    "df_sample = df.sample(frac=1, random_state=42)\n",
    "\n",
    "y = df_sample['CO2_uptake_P0.15bar_T298K [mmol/g]']\n",
    "X_df = df_sample.drop(columns=['CO2_uptake_P0.15bar_T298K [mmol/g]'])\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Apply the scaler to the numerical columns\n",
    "X = scaler.fit_transform(X_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrames to tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multilayer Perceptron\n",
      "Input size: 8\n",
      "Linear(in_features=8, out_features=64, bias=True)\n",
      "ReLU()\n",
      "Linear(in_features=64, out_features=32, bias=True)\n",
      "ReLU()\n",
      "Linear(in_features=32, out_features=1, bias=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_size = X.shape[1]\n",
    "\n",
    "'''\n",
    "    Multilayer Perceptron\n",
    "'''\n",
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, 64), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Forward pass'''\n",
    "        return self.layers(x)\n",
    "    \n",
    "    def __str__(self):\n",
    "        '''String representation of the model'''\n",
    "        model_str = \"Multilayer Perceptron\\n\"\n",
    "        model_str += \"Input size: {}\\n\".format(input_size)\n",
    "        for layer in self.layers:\n",
    "            model_str += \"{}\\n\".format(layer)\n",
    "        return model_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Training loss: 0.193, Testing loss: 0.164\n",
      "Epoch 2\n",
      "Training loss: 0.153, Testing loss: 0.139\n",
      "Epoch 3\n",
      "Training loss: 0.134, Testing loss: 0.125\n",
      "Epoch 4\n",
      "Training loss: 0.123, Testing loss: 0.118\n",
      "Epoch 5\n",
      "Training loss: 0.117, Testing loss: 0.112\n",
      "Epoch 6\n",
      "Training loss: 0.112, Testing loss: 0.108\n",
      "Epoch 7\n",
      "Training loss: 0.110, Testing loss: 0.106\n",
      "Epoch 8\n",
      "Training loss: 0.108, Testing loss: 0.105\n",
      "Epoch 9\n",
      "Training loss: 0.106, Testing loss: 0.103\n",
      "Epoch 10\n",
      "Training loss: 0.105, Testing loss: 0.101\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "# Custom Dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "def save_report(data, date):\n",
    "    try:\n",
    "        os.mkdir('../reports')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    filename = f'../report/report_{date}.json'\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "learning_rate = 1e-4\n",
    "epochs = 10\n",
    "patience = 2  # Number of epochs to wait for improvement\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = CustomDataset(X_tensor, y_tensor)\n",
    "trainloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "test_dataset = CustomDataset(X_test, y_test)\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# set fixed random seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Initializing Net\n",
    "mlp = MLP()\n",
    "\n",
    "# defining loss function\n",
    "loss_function = nn.MSELoss()\n",
    "# defining optimizer\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=learning_rate)\n",
    "\n",
    "# Early stopping variables\n",
    "epochs_no_improve = 0\n",
    "\n",
    "training_loss = 0.0\n",
    "best_test_loss = 0.0\n",
    "best_epoch = 0.0\n",
    "\n",
    "# training loop\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    print('Epoch %s' % (epoch+1))\n",
    "\n",
    "    current_loss = 0.0\n",
    "    \n",
    "\n",
    "    # iterate over the data\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        \n",
    "        # get data and ground truth\n",
    "        inputs, targets = data\n",
    "\n",
    "        # set gradients of all optimized tensors to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass of data through net\n",
    "        outputs = mlp(inputs)\n",
    "\n",
    "        # compute loss\n",
    "        loss = loss_function(outputs, targets)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # optimizing parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # show stats\n",
    "        current_loss += loss.item()\n",
    "\n",
    "    mean_loss = current_loss / len(trainloader)\n",
    "    # Testing phase\n",
    "    mlp.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(testloader, 0):\n",
    "            inputs, targets = data\n",
    "            outputs = mlp(inputs)\n",
    "            loss = loss_function(outputs, targets)\n",
    "            test_loss += loss.item()\n",
    "    \n",
    "    mean_test_loss = test_loss / len(testloader)\n",
    "\n",
    "    # Check for early stopping\n",
    "    if mean_test_loss < best_test_loss or epoch == 0:\n",
    "        best_test_loss = mean_test_loss\n",
    "        best_epoch = epoch\n",
    "        training_loss = mean_loss\n",
    "    \n",
    "    print('Training loss: %.3f, Testing loss: %.3f' % (mean_loss, mean_test_loss))\n",
    "\n",
    "\n",
    "date = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "data = {\n",
    "        'epochs': best_epoch,\n",
    "        'training_loss': training_loss,\n",
    "        'testing_loss': best_test_loss,\n",
    "        'date': date,\n",
    "        'architecture': str(mlp).split('\\n')\n",
    "}\n",
    "\n",
    "save_report(data, date)\n",
    "\n",
    "# End\n",
    "print('Training finished!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
